{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_TR_LDA_MULTICORE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00Lr_Jqcbw4"
      },
      "source": [
        "#https://towardsdatascience.com/nlp-extracting-the-main-topics-from-your-dataset-using-lda-in-minutes-21486f5aa925"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHbT8jzwdB8M"
      },
      "source": [
        "# Turkish Stemmer Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x3BWPJLNDWN2",
        "outputId": "f2d4dced-845d-47aa-ebc8-6984fc5aab32"
      },
      "source": [
        "from snowballstemmer import TurkishStemmer\n",
        "turkStem=TurkishStemmer()\n",
        "turkStem.stemWord(\"gelmişti\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gelmiş'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dmttZO0dNIF"
      },
      "source": [
        "# Import Libraries and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JwfBH3pcc7k",
        "outputId": "33f24fcd-c642-419f-a902-f78da44d59ff"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import re\n",
        "import warnings\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "WPT = nltk.WordPunctTokenizer()\n",
        "stop_word_list = nltk.corpus.stopwords.words('turkish')\n",
        "\n",
        "#Import Data\n",
        "data = pd.read_excel('/content/drive/MyDrive/turkish_topic_modelling.xlsx')\n",
        "print(data.head(10))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "       Class                                           Document\n",
            "0  astronomi  NGC 5713 Başak takımyıldızı bölgesinde bulunan...\n",
            "1  astronomi  Birçok katalogda sarmal gökada olarak sınıflan...\n",
            "2    hükümet  Corina Casanova , İsviçre Federal Şansölyesidir .\n",
            "3       yasa  Casanova , İsviçre Federal Yüksek Mahkemesi es...\n",
            "4    hükümet       Corina Casanova bir federal parlementerdir .\n",
            "5      bölge  Casanova , Hristiyan Demokrat Halk Partisi üye...\n",
            "6    hükümet  İsviçre Dışişleri Bakanlığı , İsviçre federal ...\n",
            "7    hükümet  İsviçre'nin dış ilişkilerini sürdürmekle görev...\n",
            "8      bölge  Gilgit Baltistan veya ( Pakistan Kuzey Bölgele...\n",
            "9      bölge  72496 km alan kaplamakta ve oldukça dağlık bir...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubw2zeeJdWST"
      },
      "source": [
        "Checking data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQkaobKYFO-7",
        "outputId": "c4fefe59-76c3-43be-8288-74ab19f564fe"
      },
      "source": [
        "df=data['Document']\n",
        "df"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         NGC 5713 Başak takımyıldızı bölgesinde bulunan...\n",
              "1         Birçok katalogda sarmal gökada olarak sınıflan...\n",
              "2         Corina Casanova , İsviçre Federal Şansölyesidir .\n",
              "3         Casanova , İsviçre Federal Yüksek Mahkemesi es...\n",
              "4              Corina Casanova bir federal parlementerdir .\n",
              "                                ...                        \n",
              "647804    24 yaşındaki 1.86 metre boyundaki Senegalli go...\n",
              "647805    Senegalli forvet Sion takımıyla 2009 2010 sezo...\n",
              "647806    Sahadan 2 2'lik skorla ayrılan Sion , Fenerbah...\n",
              "647807    IP2bölge kullanıcılara aynı adla bir yazılım d...\n",
              "647808    IP2bölge ziyaretçilerin IP adresi ( ülke , böl...\n",
              "Name: Document, Length: 647809, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Y7zGBScr_W"
      },
      "source": [
        "#!pip install gensim\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "#from gensim.parsing.preprocessing import STOPWORDS           #Does not support Turkish yet.\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(400)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So4TOfy5cvoG",
        "outputId": "1ca49c6b-9a20-4707-bc6a-f9add7c15070"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFHBQtOEczgd",
        "outputId": "7154c3ab-4cdd-4906-ca10-e05238808736"
      },
      "source": [
        "print(WordNetLemmatizer().lemmatize('gelmişti', pos = 'v')) # unfortunately, not working for Turkish"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gelmişti\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "8YWoGC_Tc0xF",
        "outputId": "c564b944-e68c-4af2-cf74-657269fcba1c"
      },
      "source": [
        "import pandas as pd\n",
        "from snowballstemmer import TurkishStemmer\n",
        "stemmer=TurkishStemmer()\n",
        "original_words = ['Başarılı', 'insanlar', 'adamlar', 'öldüler', 'içindekiler','kapısındaki', 'yiyecekler,', 'çıkaranlar', \n",
        "           'lahanalar', 'takımların','sırası', 'futbolcuların', 'yedikleri']\n",
        "singles = [stemmer.stemWord(plural) for plural in original_words]\n",
        "\n",
        "pd.DataFrame(data={'original word':original_words, 'stemmed':singles })"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original word</th>\n",
              "      <th>stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Başarılı</td>\n",
              "      <td>Başarıl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>insanlar</td>\n",
              "      <td>in</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>adamlar</td>\n",
              "      <td>adam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>öldüler</td>\n",
              "      <td>öl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>içindekiler</td>\n",
              "      <td>içindeki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>kapısındaki</td>\n",
              "      <td>kapıs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>yiyecekler,</td>\n",
              "      <td>yiyecekler,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>çıkaranlar</td>\n",
              "      <td>çıkaran</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>lahanalar</td>\n",
              "      <td>lahana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>takımların</td>\n",
              "      <td>tak</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>sırası</td>\n",
              "      <td>sıras</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>futbolcuların</td>\n",
              "      <td>futbolcu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>yedikleri</td>\n",
              "      <td>yedik</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    original word      stemmed\n",
              "0        Başarılı      Başarıl\n",
              "1        insanlar           in\n",
              "2         adamlar         adam\n",
              "3         öldüler           öl\n",
              "4     içindekiler     içindeki\n",
              "5     kapısındaki        kapıs\n",
              "6     yiyecekler,  yiyecekler,\n",
              "7      çıkaranlar      çıkaran\n",
              "8       lahanalar       lahana\n",
              "9      takımların          tak\n",
              "10         sırası        sıras\n",
              "11  futbolcuların     futbolcu\n",
              "12      yedikleri        yedik"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vkaqm4DmeAVf"
      },
      "source": [
        "• As it is seen, stemmer is not perfect but it works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEGdW_fweTir"
      },
      "source": [
        "# Stemming and Tokenizing Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ISKDEpOc4ZC"
      },
      "source": [
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stemWord(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "# Tokenize and lemmatize\n",
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in stop_word_list and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "     \n",
        "    return result"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40FhnUkfc7xM",
        "outputId": "f82f2a79-835a-4575-fde1-ff72d51084a2"
      },
      "source": [
        "doc_sample = 'Bu disk çok fazla kez hata verdi. Mümkünse bunu başka bir tane ile değiştirmek istiyorum.'\n",
        "\n",
        "print(\"Original document: \")\n",
        "words = []\n",
        "for word in doc_sample.split(' '):\n",
        "    words.append(word)\n",
        "print(words)\n",
        "print(\"\\n\\nTokenized and lemmatized document: \")\n",
        "print(preprocess(doc_sample))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original document: \n",
            "['Bu', 'disk', 'çok', 'fazla', 'kez', 'hata', 'verdi.', 'Mümkünse', 'bunu', 'başka', 'bir', 'tane', 'ile', 'değiştirmek', 'istiyorum.']\n",
            "\n",
            "\n",
            "Tokenized and lemmatized document: \n",
            "['disk', 'faz', 'ha', 'ver', 'mümk', 'p', 'başka', 'tane', 'değiştirmek', 'istiyor']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ3XYckrepc0"
      },
      "source": [
        "• Some of words are misstemmed (\"hata\" --> ha etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPFA1Bxsc8_P"
      },
      "source": [
        "processed_docs = []\n",
        "\n",
        "for doc in data['Document']:\n",
        "    processed_docs.append(preprocess(doc))"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Xpqj9cdCSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fee15c4-bc6e-45c9-9f0d-2927e56c9f07"
      },
      "source": [
        "print(processed_docs[:2])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['başak', 'takımyıldız', 'bölge', 'buluna', 'tuhaf', 'asimetrik', 'göka'], ['birçok', 'katalogu', 'sarmal', 'göka', 'olarak', 'sınıflandırıla', 'sarmal', 'gökada', 'oldukça', 'farklı']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuL3FRyOe9KT"
      },
      "source": [
        "# Crating Dictionary for LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSbJvCU5dCvG"
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_docs)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3U8ie4B5dEhp",
        "outputId": "7cca5c10-32ba-4edd-b0f1-359f12ddb0fb"
      },
      "source": [
        "count = 0                         #Testing if it is created\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 asimetrik\n",
            "1 başak\n",
            "2 buluna\n",
            "3 bölge\n",
            "4 göka\n",
            "5 takımyıldız\n",
            "6 tuhaf\n",
            "7 birçok\n",
            "8 farklı\n",
            "9 gökada\n",
            "10 katalogu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7xKh-5SdGc2"
      },
      "source": [
        "'''\n",
        "Remove very rare and very common words: (Probably not working for Turkish but I tried)\n",
        "'''\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBXiJ_TDdWml"
      },
      "source": [
        "'''\n",
        "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
        "words and how many times those words appear. Save this to 'bow_corpus'\n",
        "'''\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5v9LB_de-M-",
        "outputId": "02be0f0c-f1b9-43a8-a0a7-39fcbc847e17"
      },
      "source": [
        "'''\n",
        "Preview BOW for our sample preprocessed document\n",
        "'''\n",
        "document_num = 20\n",
        "bow_doc_x = bow_corpus[document_num]\n",
        "\n",
        "for i in range(len(bow_doc_x)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
        "                                                     dictionary[bow_doc_x[i][0]], \n",
        "                                                     bow_doc_x[i][1]))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 11 (\"olarak\") appears 1 time.\n",
            "Word 99 (\"leopardı\") appears 1 time.\n",
            "Word 102 (\"snow\") appears 1 time.\n",
            "Word 133 (\"intel\") appears 1 time.\n",
            "Word 138 (\"tabanlı\") appears 1 time.\n",
            "Word 145 (\"ağustos\") appears 1 time.\n",
            "Word 146 (\"bilgisayar\") appears 1 time.\n",
            "Word 147 (\"macintosh\") appears 1 time.\n",
            "Word 148 (\"tarih\") appears 1 time.\n",
            "Word 149 (\"yayınla\") appears 1 time.\n",
            "Word 150 (\"yükseltme\") appears 1 time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lb0pHTmfmto"
      },
      "source": [
        "# LDA Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnVMzA_Ce_cB"
      },
      "source": [
        "\n",
        "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
        "                                   num_topics = 20, \n",
        "                                   id2word = dictionary,                                    \n",
        "                                   passes = 10,\n",
        "                                   workers = 2)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evt2Fue5fCEt",
        "outputId": "d4d421a8-f7e7-403d-e147-ccf1bbe0e87f"
      },
      "source": [
        "'''\n",
        "For each topic, we will explore the words occuring in that topic and its relative weight\n",
        "'''\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.018*\"dil\" + 0.017*\"olarak\" + 0.013*\"özellik\" + 0.010*\"olduk\" + 0.010*\"kaynak\" + 0.008*\"yuna\" + 0.008*\"ola\" + 0.007*\"diğer\" + 0.007*\"olmas\" + 0.007*\"örnek\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.014*\"klip\" + 0.012*\"video\" + 0.011*\"çekil\" + 0.011*\"çek\" + 0.011*\"karar\" + 0.011*\"dikkat\" + 0.010*\"taraf\" + 0.009*\"be\" + 0.009*\"çizgi\" + 0.009*\"charles\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.035*\"sıra\" + 0.020*\"liste\" + 0.020*\"dünya\" + 0.018*\"üçüncü\" + 0.015*\"milyo\" + 0.014*\"büyük\" + 0.013*\"ol\" + 0.011*\"ulaş\" + 0.011*\"faz\" + 0.011*\"kadar\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.041*\"başka\" + 0.020*\"görev\" + 0.020*\"olarak\" + 0.019*\"g\" + 0.016*\"ay\" + 0.016*\"seçil\" + 0.014*\"başkanlık\" + 0.011*\"ol\" + 0.011*\"genel\" + 0.010*\"yap\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.063*\"tarih\" + 0.029*\"türk\" + 0.022*\"türki\" + 0.018*\"taraf\" + 0.016*\"stanbul\" + 0.015*\"olarak\" + 0.013*\"kurul\" + 0.012*\"ocak\" + 0.012*\"mayıs\" + 0.011*\"hazira\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.029*\"birlik\" + 0.028*\"devlet\" + 0.023*\"ülke\" + 0.019*\"avrup\" + 0.017*\"amerik\" + 0.015*\"birleşik\" + 0.014*\"taraf\" + 0.013*\"krallık\" + 0.011*\"sovyet\" + 0.011*\"olarak\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.028*\"yazar\" + 0.028*\"taraf\" + 0.022*\"kitap\" + 0.018*\"adlı\" + 0.016*\"eser\" + 0.016*\"oy\" + 0.015*\"ilçes\" + 0.013*\"roma\" + 0.013*\"böl\" + 0.010*\"tiyatro\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.034*\"üzer\" + 0.023*\"olmak\" + 0.013*\"oyun\" + 0.011*\"olimpiyat\" + 0.010*\"madalya\" + 0.010*\"yıllık\" + 0.009*\"yarışma\" + 0.009*\"al\" + 0.009*\"baş\" + 0.008*\"oy\"\n",
            "\n",
            "\n",
            "Topic: 8 \n",
            "Words: 0.025*\"güney\" + 0.021*\"dok\" + 0.021*\"kuzey\" + 0.018*\"or\" + 0.017*\"bölge\" + 0.015*\"bat\" + 0.014*\"de\" + 0.012*\"alır\" + 0.012*\"sınır\" + 0.012*\"ola\"\n",
            "\n",
            "\n",
            "Topic: 9 \n",
            "Words: 0.022*\"üniversites\" + 0.019*\"çalışma\" + 0.016*\"sonra\" + 0.015*\"okul\" + 0.014*\"olarak\" + 0.013*\"eğit\" + 0.011*\"ol\" + 0.011*\"sanat\" + 0.011*\"başladı\" + 0.009*\"paris\"\n",
            "\n",
            "\n",
            "Topic: 10 \n",
            "Words: 0.062*\"zama\" + 0.053*\"aynı\" + 0.022*\"ya\" + 0.013*\"olarak\" + 0.011*\"model\" + 0.011*\"sır\" + 0.009*\"ola\" + 0.008*\"motor\" + 0.008*\"araç\" + 0.008*\"üret\"\n",
            "\n",
            "\n",
            "Topic: 11 \n",
            "Words: 0.033*\"tak\" + 0.029*\"futbol\" + 0.027*\"maç\" + 0.019*\"ligi\" + 0.019*\"takım\" + 0.018*\"ol\" + 0.017*\"kulüp\" + 0.017*\"sezo\" + 0.017*\"kupas\" + 0.015*\"sezon\"\n",
            "\n",
            "\n",
            "Topic: 12 \n",
            "Words: 0.041*\"merkez\" + 0.040*\"bağlı\" + 0.037*\"bölge\" + 0.032*\"ilçe\" + 0.030*\"eyalet\" + 0.028*\"il\" + 0.027*\"şehir\" + 0.026*\"ola\" + 0.025*\"buluna\" + 0.017*\"kent\"\n",
            "\n",
            "\n",
            "Topic: 13 \n",
            "Words: 0.037*\"savaş\" + 0.025*\"sonra\" + 0.018*\"dünya\" + 0.015*\"gel\" + 0.015*\"karşı\" + 0.014*\"asker\" + 0.012*\"alma\" + 0.010*\"kuvvet\" + 0.010*\"sıra\" + 0.010*\"sür\"\n",
            "\n",
            "\n",
            "Topic: 14 \n",
            "Words: 0.056*\"grup\" + 0.044*\"alp\" + 0.043*\"şarkı\" + 0.030*\"müzik\" + 0.018*\"albüm\" + 0.015*\"olarak\" + 0.011*\"sonra\" + 0.010*\"adlı\" + 0.010*\"parça\" + 0.009*\"piyasa\"\n",
            "\n",
            "\n",
            "Topic: 15 \n",
            "Words: 0.023*\"yüzyıl\" + 0.022*\"dönem\" + 0.021*\"ro\" + 0.021*\"kral\" + 0.019*\"olarak\" + 0.017*\"ola\" + 0.016*\"yıldız\" + 0.014*\"oğlu\" + 0.014*\"mparatorluk\" + 0.012*\"uzaklık\"\n",
            "\n",
            "\n",
            "Topic: 16 \n",
            "Words: 0.024*\"olduk\" + 0.018*\"kendi\" + 0.016*\"ola\" + 0.013*\"sonra\" + 0.010*\"çocuk\" + 0.010*\"p\" + 0.010*\"olma\" + 0.009*\"yaş\" + 0.009*\"fakat\" + 0.009*\"o\"\n",
            "\n",
            "\n",
            "Topic: 17 \n",
            "Words: 0.033*\"kiş\" + 0.023*\"gör\" + 0.021*\"nüfus\" + 0.014*\"göl\" + 0.012*\"yüzölç\" + 0.012*\"taraf\" + 0.011*\"topla\" + 0.011*\"gitar\" + 0.009*\"tanrı\" + 0.009*\"tar\"\n",
            "\n",
            "\n",
            "Topic: 18 \n",
            "Words: 0.090*\"film\" + 0.039*\"ödül\" + 0.027*\"yap\" + 0.025*\"filmi\" + 0.024*\"al\" + 0.021*\"oyuncu\" + 0.016*\"televizyo\" + 0.014*\"kaza\" + 0.013*\"yönetme\" + 0.010*\"sinema\"\n",
            "\n",
            "\n",
            "Topic: 19 \n",
            "Words: 0.050*\"olarak\" + 0.012*\"ola\" + 0.010*\"kabul\" + 0.010*\"farklı\" + 0.010*\"genellik\" + 0.008*\"biline\" + 0.008*\"edile\" + 0.008*\"edilir\" + 0.007*\"büyük\" + 0.007*\"yük\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EW5DjX9fsDm"
      },
      "source": [
        "# Unseen Text Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST0eQNBTfkRF",
        "outputId": "5e822e01-a3c0-48de-8c06-fcfe92bb1477"
      },
      "source": [
        "unseen_document = \"Onun müzik kulağı yeteneklerin de ötesindedir. Keman gitar piyano ve daha birçok enstrümanın üstadıdır dersek yeridir\"\n",
        "print(unseen_document)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Onun müzik kulağı yeteneklerin de ötesindedir. Keman gitar piyano ve daha birçok enstrümanın üstadıdır dersek yeridir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbpZqduLfksi",
        "outputId": "29cc99e8-05d6-44c4-9fa8-773f1f1bc8d2"
      },
      "source": [
        "# Data preprocessing step for the unseen document\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.25007668137550354\t Topic: 0.056*\"grup\" + 0.044*\"alp\" + 0.043*\"şarkı\" + 0.030*\"müzik\" + 0.018*\"albüm\"\n",
            "Score: 0.20905724167823792\t Topic: 0.024*\"olduk\" + 0.018*\"kendi\" + 0.016*\"ola\" + 0.013*\"sonra\" + 0.010*\"çocuk\"\n",
            "Score: 0.18945811688899994\t Topic: 0.050*\"olarak\" + 0.012*\"ola\" + 0.010*\"kabul\" + 0.010*\"farklı\" + 0.010*\"genellik\"\n",
            "Score: 0.07775552570819855\t Topic: 0.025*\"güney\" + 0.021*\"dok\" + 0.021*\"kuzey\" + 0.018*\"or\" + 0.017*\"bölge\"\n",
            "Score: 0.07722388207912445\t Topic: 0.063*\"tarih\" + 0.029*\"türk\" + 0.022*\"türki\" + 0.018*\"taraf\" + 0.016*\"stanbul\"\n",
            "Score: 0.07500000298023224\t Topic: 0.014*\"klip\" + 0.012*\"video\" + 0.011*\"çekil\" + 0.011*\"çek\" + 0.011*\"karar\"\n",
            "Score: 0.07500000298023224\t Topic: 0.033*\"kiş\" + 0.023*\"gör\" + 0.021*\"nüfus\" + 0.014*\"göl\" + 0.012*\"yüzölç\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nazC3B6Efx90"
      },
      "source": [
        "Results are not bad. Even if there are nonesense words (because of stemming) it works generally.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}