{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_TR_LDA_MULTICORE(2).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWd7Y1AVkh3N"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import warnings\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import nltk\n",
        "!pip install progressbar2\n",
        "from progressbar import ProgressBar\n",
        "bar = ProgressBar()\n",
        "nltk.download('stopwords')\n",
        "WPT = nltk.WordPunctTokenizer()\n",
        "stop_word_list = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "#Import Data\n",
        "dataset =pd.read_csv(\"/content/drive/MyDrive/clean_data.csv\")\n",
        "data = dataset['0']\n",
        "print(data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_t8zRqXk97a",
        "outputId": "6dd91b74-cd47-4192-f63f-05f2baaf84ad"
      },
      "source": [
        "!pip install gensim\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2vfKXuylC8O",
        "outputId": "ef642ac3-ad47-458f-89d7-8f10ea9a959b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import pandas as pd\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer=SnowballStemmer('english')"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dIIaH2SlKHg"
      },
      "source": [
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in stop_word_list and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "     \n",
        "    return result\n",
        "\n",
        "def clean_data(data):\n",
        "  data = data.replace('\\n',' ')\n",
        "  data = data.replace('\\t',' ')\n",
        "  data = data.map(lambda x: re.sub('[,\\.\\'!?();:$%&#\"]', '', x))\n",
        "  data = data.replace('\\n','', regex=True)\n",
        "  data = data.replace('\\'','', regex=True)\n",
        "  data = data.replace('-','', regex=True)\n",
        "  data = data.replace('’','', regex=True)\n",
        "  \n",
        "  return data"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7L07974lSYi",
        "outputId": "070a4d83-7e61-4457-bead-0c496802c048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import string\n",
        "converted_list = []\n",
        "data.astype(str)\n",
        "data = data.astype(str) \n",
        "data=clean_data(data)\n",
        "\n",
        "for element in data:\n",
        "    element = element.translate(str.maketrans('','',string.punctuation))\n",
        "    converted_list.append(element.strip())\n",
        "\n",
        "processed_docs = []\n",
        "\n",
        "for doc in bar(converted_list):\n",
        "    processed_docs.append(preprocess(doc))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% (49999 of 49999) |##################| Elapsed Time: 0:08:06 Time:  0:08:06\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8WuLHBolUHD",
        "outputId": "0119d5a0-dd4a-4f77-c94b-5c4a99f7d405"
      },
      "source": [
        "bigram = gensim.models.Phrases(processed_docs, min_count=5, threshold=100)\n",
        "trigram = gensim.models.Phrases(bigram[processed_docs], threshold=100)  \n",
        "\n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "print(trigram_mod[bigram_mod[processed_docs[0]]])\n",
        "\n",
        "def make_bigrams(texts):\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def make_trigrams(texts):\n",
        "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    texts_out = []\n",
        "    for sent in bar(texts):\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['content', 'patriot', 'peter_berg', 'thriller', 'recreat', 'boston_marathon_bomb', 'ensu', 'manhunt', 'follow', 'surpris', 'obliqu', 'moral', 'ambigu', 'movi', 'typic', 'straightforward', 'filmmak', 'patriot', 'take', 'unexpect', 'cynic', 'view', 'chao', 'rash', 'bureaucrat_infight', 'follow', 'bomb', 'question', 'whether', 'berg', 'intend', 'messag', 'grim', 'run', 'time', 'movi', 'celebr', 'grind', 'help', 'bring', 'bomber', 'justic', 'glimps', 'someth', 'complic', 'jingoism', 'realli', 'linger', 'scene', 'good', 'illustr', 'dichotomi', 'come', 'late', 'patriot', 'search', 'bomber_dzhokhar_tsarnaev', 'intensifi', 'brother', 'wife', 'katherin', 'melissa', 'benoist', 'bring', 'interrog', 'connect', 'case', 'lawyer', 'protest', 'right', 'steeli', 'interview', 'khandi', 'alexand', 'honey', 'shit', 'hollywood', 'line', 'intend', 'provok', 'cheer', 'audienc', 'despit', 'queasi', 'legal', 'underton', 'film', 'present', 'cheesi', 'line', 'question', 'nowher', 'interrog', 'exit', 'shrug', 'movi', 'note', 'katherin', 'never', 'charg', 'crime', 'patriot', 'fill', 'kind', 'bluster', 'film', 'compel', 'moment', 'serv', 'undercut', 'deepwat_horizon', 'stay', 'close', 'surfac', 'work', 'interest', 'theme', 'center', 'movi', 'boston', 'polic', 'sergeant', 'tommi', 'saunder', 'fiction', 'charact', 'invent', 'serv', 'audienc', 'everi', 'major', 'turn', 'point', 'attack', 'play', 'mark_wahlberg', 'polestar', 'berg', 'genr', 'storytel', 'play', 'mouthi', 'hero', 'center', 'lone', 'survivor', 'deepwat_horizon', 'patriot', 'unlik', 'film', 'wahlberg', 'charact', 'patriot', 'realli', 'exist', 'often', 'come', 'across', 'bundl', 'screenwrit', 'cliché', 'nurs', 'knee', 'recent', 'demot', 'insubordin', 'somehow', 'term', 'everi', 'uniform', 'otherwis', 'citi', 'film', 'focus', 'saunder', 'most', 'feel', 'like', 'straight', 'recreat', 'traumat', 'event', 'occur', 'less', 'four', 'year', 'berg', 'larg', 'well', 'avoid', 'sensation', 'bomb', 'kill', 'wind', 'subsequ', 'manhunt', 'claim', 'life', 'polic', 'offic', 'point', 'patriot', 'feel', 'like', 'glitzi', 'action', 'movi', 'gori', 'piec', 'horror', 'violenc', 'linger', 'outsid', 'effort', 'make', 'victim', 'feel', 'like', 'real', 'peopl', 'depict', 'snippet', 'life', 'beforehand', 'wahlberg', 'part', 'help', 'make', 'saunder', 'feel', 'like', 'stereotyp', 'movi', 'film', 'otherwis', 'seek', 'nuanc', 'saunder', 'seem', 'suppos', 'stand', 'general', 'lovabl', 'belliger', 'boston', 'spirit', 'prevail', 'marathon', 'berg', 'drill', 'point', 'home', 'mani', 'time', 'work', 'immedi', 'aftermath', 'bomb', 'wahlberg', 'success', 'communic', 'angri', 'shake', 'saunder', 'rather', 'switch', 'hero', 'mode', 'decent', 'sell', 'trauma', 'hefti', 'attitud', 'return', 'patriot', 'snap', 'back', 'sever', 'unreal', 'sinc', 'film', 'fascin', 'faith', 'convey', 'confus', 'bomb', 'differ', 'opinion', 'clash', 'ego', 'lead', 'agent', 'richard', 'deslauri', 'kevin_bacon', 'take', 'case', 'recreat', 'block', 'attack', 'bomb', 'insid', 'gigant', 'warehous', 'comb', 'surveil', 'video', 'clue', 'boston', 'polic', 'commission', 'davi', 'john_goodman', 'massachusett', 'governor', 'deval_patrick', 'michael', 'beach', 'boston', 'mayor', 'thoma', 'menino', 'vincent', 'curatola', 'gather', 'offer', 'guidanc', 'case', 'saunder', 'present', 'reason', 'most', 'provid', 'expertis', 'busi', 'around', 'boylston', 'street', 'thorni', 'mucki', 'debat', 'play', 'manhunt', 'good', 'part', 'film', 'berg', 'accur', 'show', 'deslauri', 'agon', 'whether', 'releas', 'pictur', 'tsarnaev_brother', 'glean', 'surveil', 'camera', 'fulli', 'substanti', 'bomber', 'hand', 'eventu', 'forc', 'leak', 'medium', 'action', 'seri', 'event', 'lead', 'dead', 'shootout', 'tsarnaev_brother', 'watertown', 'simmon', 'play', 'polic', 'sergeant', 'berg', 'wise', 'delv', 'utter', 'chao', 'manhunt', 'includ', 'brief', 'inform', 'declar', 'martial', 'bizarr', 'bloodi', 'standoff', 'brother', 'pipe_bomb', 'bedlam', 'word', 'total', 'disord', 'mad', 'kind', 'violenc', 'fear', 'terror', 'intend', 'provok', 'even', 'stop', 'boston', 'effort', 'captur', 'tsarnaev_brother', 'just', 'depict', 'heroic', 'craze', 'random', 'chain', 'event', 'empti', 'angri', 'postur', 'stick', 'worthi', 'analysi', 'berg', 'deeper', 'could', 'great', 'film', 'hand', 'stand', 'deliv', 'rote', 'occasion', 'misfir', 'name', 'dtype_object']\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.7/dist-packages (3.38.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from progressbar2) (1.15.0)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2) (2.5.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNBGWwSSlXw1"
      },
      "source": [
        "import spacy\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
        "\n",
        "data_words_trigrams = make_trigrams(processed_docs)\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewd5i8m5Fv-F"
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(data_lemmatized)"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHjsGuuuG00P",
        "outputId": "cb5fc0e4-cab5-4179-927a-930ae66667f4"
      },
      "source": [
        "count = 0                         #Testing if it is created\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 attack\n",
            "1 audienc\n",
            "2 avoid\n",
            "3 block\n",
            "4 bomb\n",
            "5 bomber\n",
            "6 bring\n",
            "7 brother\n",
            "8 bureaucrat_infight\n",
            "9 case\n",
            "10 celebr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saf8j5JXG8Af"
      },
      "source": [
        "'''\n",
        "Remove very rare and very common words: (Probably not working for Turkish but I tried)\n",
        "'''\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 100000)"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSynY0BcG-hL"
      },
      "source": [
        "'''\n",
        "Create the Bag-of-words model for each document i.e for each document we create a dictionary reporting how many\n",
        "words and how many times those words appear. Save this to 'bow_corpus'\n",
        "'''\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in data_lemmatized]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCc6MISdHC5k"
      },
      "source": [
        "'''\n",
        "Preview BOW for our sample preprocessed document\n",
        "'''\n",
        "document_num = 20\n",
        "bow_doc_x = bow_corpus[document_num]\n",
        "\n",
        "for i in range(len(bow_doc_x)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
        "                                                     dictionary[bow_doc_x[i][0]], \n",
        "                                                     bow_doc_x[i][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8StGIIpHD52"
      },
      "source": [
        "\n",
        "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
        "                                   num_topics = 30, \n",
        "                                   id2word = dictionary,                                    \n",
        "                                   passes = 10,\n",
        "                                   workers = 2)"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju0x4sBHHJwO",
        "outputId": "c28fcf4c-bc3e-4f7d-fde5-db272b8777b7"
      },
      "source": [
        "'''\n",
        "For each topic, we will explore the words occuring in that topic and its relative weight\n",
        "'''\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.057*\"investig\" + 0.025*\"document\" + 0.024*\"agent\" + 0.020*\"enforc\" + 0.016*\"prosecutor\" + 0.015*\"arrest\" + 0.015*\"file\" + 0.012*\"crime\" + 0.012*\"request\" + 0.012*\"agenc\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.053*\"video\" + 0.035*\"facebook\" + 0.030*\"twitter\" + 0.027*\"site\" + 0.024*\"network\" + 0.024*\"user\" + 0.017*\"platform\" + 0.017*\"social_medium\" + 0.016*\"internet\" + 0.012*\"post\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.031*\"worker\" + 0.026*\"cost\" + 0.022*\"rate\" + 0.020*\"program\" + 0.013*\"fund\" + 0.011*\"budget\" + 0.010*\"employ\" + 0.009*\"benefit\" + 0.009*\"poor\" + 0.008*\"wage\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.007*\"religi\" + 0.007*\"movement\" + 0.007*\"moral\" + 0.007*\"free\" + 0.007*\"civil\" + 0.006*\"speech\" + 0.006*\"fear\" + 0.006*\"progress\" + 0.005*\"freedom\" + 0.005*\"muslim\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.029*\"russian\" + 0.022*\"press\" + 0.015*\"journalist\" + 0.015*\"fire\" + 0.014*\"investig\" + 0.013*\"medium\" + 0.013*\"director\" + 0.012*\"comey\" + 0.012*\"spicer\" + 0.009*\"leak\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.043*\"market\" + 0.037*\"trade\" + 0.020*\"growth\" + 0.018*\"investor\" + 0.017*\"global\" + 0.017*\"economi\" + 0.016*\"chine\" + 0.015*\"rise\" + 0.014*\"invest\" + 0.014*\"stock\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.056*\"music\" + 0.025*\"band\" + 0.023*\"artist\" + 0.020*\"perform\" + 0.016*\"song\" + 0.015*\"stage\" + 0.014*\"singer\" + 0.013*\"sing\" + 0.013*\"sound\" + 0.013*\"listen\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.106*\"school\" + 0.103*\"student\" + 0.058*\"parent\" + 0.030*\"teacher\" + 0.018*\"class\" + 0.016*\"campus\" + 0.014*\"teach\" + 0.014*\"program\" + 0.013*\"educ\" + 0.009*\"attend\"\n",
            "\n",
            "\n",
            "Topic: 8 \n",
            "Words: 0.027*\"debat\" + 0.021*\"senat\" + 0.012*\"speech\" + 0.012*\"confirm\" + 0.012*\"convent\" + 0.011*\"endor\" + 0.009*\"nomine\" + 0.009*\"presid\" + 0.008*\"remark\" + 0.007*\"stage\"\n",
            "\n",
            "\n",
            "Topic: 9 \n",
            "Words: 0.114*\"race\" + 0.030*\"contest\" + 0.022*\"gender\" + 0.019*\"convent\" + 0.014*\"texa\" + 0.013*\"rubio\" + 0.011*\"primari\" + 0.011*\"finish\" + 0.011*\"cruz\" + 0.010*\"beat\"\n",
            "\n",
            "\n",
            "Topic: 10 \n",
            "Words: 0.042*\"game\" + 0.041*\"season\" + 0.038*\"player\" + 0.011*\"goal\" + 0.010*\"giant\" + 0.009*\"defen\" + 0.009*\"score\" + 0.008*\"pick\" + 0.008*\"coach\" + 0.008*\"sport\"\n",
            "\n",
            "\n",
            "Topic: 11 \n",
            "Words: 0.094*\"email\" + 0.042*\"donat\" + 0.023*\"sander\" + 0.023*\"palestinian\" + 0.022*\"hillari\" + 0.019*\"aid\" + 0.019*\"isra\" + 0.014*\"jewish\" + 0.013*\"scandal\" + 0.012*\"settlement\"\n",
            "\n",
            "\n",
            "Topic: 12 \n",
            "Words: 0.061*\"voter\" + 0.015*\"poll\" + 0.010*\"popular\" + 0.010*\"elect\" + 0.009*\"race\" + 0.009*\"elector\" + 0.008*\"favor\" + 0.007*\"establish\" + 0.007*\"seat\" + 0.006*\"survey\"\n",
            "\n",
            "\n",
            "Topic: 13 \n",
            "Words: 0.113*\"letter\" + 0.038*\"fraud\" + 0.029*\"cuban\" + 0.027*\"mayor\" + 0.024*\"card\" + 0.013*\"mail\" + 0.013*\"chair\" + 0.012*\"spanish\" + 0.011*\"ballot\" + 0.010*\"recount\"\n",
            "\n",
            "\n",
            "Topic: 14 \n",
            "Words: 0.040*\"polic\" + 0.024*\"murder\" + 0.023*\"death\" + 0.022*\"fire\" + 0.016*\"dead\" + 0.015*\"arrest\" + 0.013*\"crime\" + 0.013*\"protest\" + 0.012*\"video\" + 0.011*\"suspect\"\n",
            "\n",
            "\n",
            "Topic: 15 \n",
            "Words: 0.052*\"court\" + 0.033*\"lawyer\" + 0.030*\"legal\" + 0.019*\"suprem_court\" + 0.018*\"trial\" + 0.018*\"file\" + 0.013*\"attorney\" + 0.012*\"lawsuit\" + 0.012*\"protect\" + 0.011*\"appeal\"\n",
            "\n",
            "\n",
            "Topic: 16 \n",
            "Words: 0.028*\"water\" + 0.021*\"area\" + 0.019*\"land\" + 0.018*\"town\" + 0.017*\"driver\" + 0.010*\"site\" + 0.009*\"crash\" + 0.009*\"drive\" + 0.009*\"road\" + 0.008*\"train\"\n",
            "\n",
            "\n",
            "Topic: 17 \n",
            "Words: 0.046*\"film\" + 0.017*\"actor\" + 0.011*\"charact\" + 0.010*\"role\" + 0.009*\"scene\" + 0.009*\"season\" + 0.009*\"episod\" + 0.008*\"director\" + 0.008*\"screen\" + 0.007*\"novel\"\n",
            "\n",
            "\n",
            "Topic: 18 \n",
            "Words: 0.015*\"scientist\" + 0.014*\"planet\" + 0.014*\"human\" + 0.013*\"environment\" + 0.010*\"plant\" + 0.010*\"water\" + 0.010*\"space\" + 0.009*\"project\" + 0.008*\"robot\" + 0.008*\"warm\"\n",
            "\n",
            "\n",
            "Topic: 19 \n",
            "Words: 0.022*\"refuge\" + 0.015*\"syrian\" + 0.015*\"german\" + 0.015*\"soldier\" + 0.015*\"forc\" + 0.013*\"bomb\" + 0.011*\"strike\" + 0.011*\"region\" + 0.011*\"fighter\" + 0.010*\"terrorist\"\n",
            "\n",
            "\n",
            "Topic: 20 \n",
            "Words: 0.028*\"wear\" + 0.011*\"walk\" + 0.010*\"dress\" + 0.009*\"flight\" + 0.008*\"trip\" + 0.008*\"wait\" + 0.008*\"attend\" + 0.008*\"travel\" + 0.008*\"seat\" + 0.007*\"front\"\n",
            "\n",
            "\n",
            "Topic: 21 \n",
            "Words: 0.008*\"kind\" + 0.008*\"reader\" + 0.007*\"book\" + 0.007*\"alway\" + 0.007*\"wrong\" + 0.007*\"mind\" + 0.007*\"sen\" + 0.007*\"relationship\" + 0.006*\"cour\" + 0.006*\"whole\"\n",
            "\n",
            "\n",
            "Topic: 22 \n",
            "Words: 0.039*\"senat\" + 0.033*\"bill\" + 0.027*\"immigr\" + 0.019*\"border\" + 0.018*\"reform\" + 0.012*\"pass\" + 0.011*\"propos\" + 0.009*\"legisl\" + 0.009*\"congression\" + 0.008*\"push\"\n",
            "\n",
            "\n",
            "Topic: 23 \n",
            "Words: 0.031*\"phone\" + 0.017*\"page\" + 0.017*\"datum\" + 0.015*\"access\" + 0.013*\"search\" + 0.012*\"email\" + 0.011*\"date\" + 0.011*\"text\" + 0.010*\"publish\" + 0.009*\"account\"\n",
            "\n",
            "\n",
            "Topic: 24 \n",
            "Words: 0.080*\"black\" + 0.020*\"studi\" + 0.018*\"research\" + 0.014*\"human\" + 0.009*\"brain\" + 0.008*\"racial\" + 0.007*\"color\" + 0.007*\"cell\" + 0.006*\"race\" + 0.006*\"male\"\n",
            "\n",
            "\n",
            "Topic: 25 \n",
            "Words: 0.034*\"prison\" + 0.028*\"sexual\" + 0.027*\"girl\" + 0.021*\"rape\" + 0.017*\"victim\" + 0.015*\"jail\" + 0.015*\"femal\" + 0.014*\"sexual_assault\" + 0.011*\"crime\" + 0.010*\"assault\"\n",
            "\n",
            "\n",
            "Topic: 26 \n",
            "Words: 0.022*\"food\" + 0.012*\"room\" + 0.011*\"drink\" + 0.008*\"design\" + 0.007*\"floor\" + 0.007*\"hair\" + 0.005*\"apart\" + 0.005*\"water\" + 0.005*\"cloth\" + 0.004*\"skin\"\n",
            "\n",
            "\n",
            "Topic: 27 \n",
            "Words: 0.021*\"foreign\" + 0.018*\"threat\" + 0.013*\"secur\" + 0.012*\"travel\" + 0.011*\"terrorist\" + 0.011*\"terror\" + 0.010*\"agreement\" + 0.010*\"defen\" + 0.009*\"north_korea\" + 0.009*\"nuclear\"\n",
            "\n",
            "\n",
            "Topic: 28 \n",
            "Words: 0.046*\"doctor\" + 0.039*\"session\" + 0.029*\"patient\" + 0.028*\"drug\" + 0.022*\"test\" + 0.022*\"health\" + 0.020*\"cancer\" + 0.019*\"risk\" + 0.019*\"treatment\" + 0.018*\"hospit\"\n",
            "\n",
            "\n",
            "Topic: 29 \n",
            "Words: 0.029*\"sell\" + 0.026*\"price\" + 0.026*\"sale\" + 0.017*\"market\" + 0.016*\"product\" + 0.016*\"store\" + 0.015*\"owner\" + 0.013*\"brand\" + 0.013*\"retail\" + 0.011*\"busi\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U93XgPC9HOUl"
      },
      "source": [
        "from pandas import DataFrame\n",
        "unseen_document = [\"\"\" India's states should declare an epidemic following a rise in deadly \"black fungus\" cases, the country's health authorities has said.\n",
        "\n",
        "The normally rare infection, called mucormycosis, has a mortality rate of 50%, with some only saved by removing an eye or jaw bone.\n",
        "\n",
        "But in recent months, India saw thousands of cases affecting recovered and recovering Covid-19 patients.\n",
        "\n",
        "Doctors suspect there may be a link with the steroids used to treat Covid.\n",
        "\n",
        "Diabetics are at particular risk, with doctors telling the BBC it seems to strike 12 to 15 days after recovery from Covid.\n",
        "\n",
        "The deadly 'black fungus' maiming Covid patients\n",
        "India's holiest river is swollen with bodies\n",
        "Six hospitals, three days and a Covid nightmare\n",
        "On Thursday, Health Ministry Joint Secretary Lav Agarwal wrote to India's 29 states to ask them to declare it an epidemic.\n",
        "\n",
        "By doing so, the ministry will be able more closely to monitor what is happening in each state, and allow for better integration of treatment.\n",
        "\n",
        "It is not clear exactly how many cases there have been across the country, which is currently in the grip of a deadly second Covid-19 wave which has left tens of thousands dead.  \"\"\"]\n",
        "\n",
        "df = DataFrame(unseen_document,columns=['text'])\n",
        "docs = df['text']\n",
        "docs=clean_data(docs)\n",
        "\n",
        "def token(values):\n",
        "    filtered_words = [word for word in values.split() if word not in stop_word_list]\n",
        "    \n",
        "    not_stopword_doc = \" \".join(filtered_words)\n",
        "    return not_stopword_doc\n",
        "docs = docs.map(lambda x: token(x))\n",
        "df = docs\n",
        "\n",
        "bow_vector = dictionary.doc2bow(preprocess(df[0]))"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4G81W2aoHQmc",
        "outputId": "a4a7f985-a274-407f-cb59-3de9fe1baed2"
      },
      "source": [
        "# Data preprocessing step for the unseen document\n",
        "count=0\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    count+=1\n",
        "    if(count<6):\n",
        "      print(\"Score: %{}\\t Topic: {}\".format(score*100, re. findall('\"([^\"]*)\"', lda_model.print_topic(index, 10))))"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: %37.302157282829285\t Topic: ['doctor', 'session', 'patient', 'drug', 'test', 'health', 'cancer', 'risk', 'treatment', 'hospit']\n",
            "Score: %23.647862672805786\t Topic: ['black', 'studi', 'research', 'human', 'brain', 'racial', 'color', 'cell', 'race', 'male']\n",
            "Score: %17.709460854530334\t Topic: ['refuge', 'syrian', 'german', 'soldier', 'forc', 'bomb', 'strike', 'region', 'fighter', 'terrorist']\n",
            "Score: %9.814301878213882\t Topic: ['polic', 'murder', 'death', 'fire', 'dead', 'arrest', 'crime', 'protest', 'video', 'suspect']\n",
            "Score: %4.485643655061722\t Topic: ['market', 'trade', 'growth', 'investor', 'global', 'economi', 'chine', 'rise', 'invest', 'stock']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}