# -*- coding: utf-8 -*-
"""LDA_99(Saved).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vjvg0OE--Q1dWuc-CDgijDI7rDM3WVGc
"""

import re
!pip install gensim
import gensim
from gensim.utils import simple_preprocess

import pickle
dictionary = gensim.corpora.Dictionary.load('/content/drive/MyDrive/zzz/dictionary.gensim')
corpus = pickle.load(open('/content/drive/MyDrive/zzz/corpus.pkl', 'rb'))
lda = gensim.models.ldamodel.LdaModel.load('/content/drive/MyDrive/zzz/model_99.gensim')

import pandas as pd
from snowballstemmer import TurkishStemmer
stemmer=TurkishStemmer()
import nltk
nltk.download('stopwords')
WPT = nltk.WordPunctTokenizer()
stop_word_list = nltk.corpus.stopwords.words('turkish')
def lemmatize_stemming(text):
    return stemmer.stemWord(text)

# Tokenize and lemmatize
def preprocess(text):
    result=[]
    for token in gensim.utils.simple_preprocess(text) :
        if token not in stop_word_list and len(token) > 3:
            result.append(lemmatize_stemming(token))
     
    return result
unseen_document = ["SpaceX, NeuraLink ve Tesla şirketlerinin sahibi olan ünlü yatırımcı Elon Musk, Mars'a gidebilmek için gerekli olan teknolojilere merak saldı. Ayrıca, hedefinin Mars'ta koloni kurup orada ölmek istediğini söyledi"]
# Data preprocessing step for the unseen document

import nltk
nltk.download('stopwords')
WPT = nltk.WordPunctTokenizer()
stop_word_list = nltk.corpus.stopwords.words('turkish')
#Daha sonra verimizdeki noktalama işaretlerini kaldırıp stopword'lerden arındırıyoruz.
from pandas import DataFrame
df = DataFrame(unseen_document,columns=['text'])
docs = df['text']
docs = docs.map(lambda x: re.sub('[,\.!?();:$%&#"]', '', x))
docs = docs.map(lambda x: x.lower())
docs = docs.map(lambda x: x.strip())
#stopword'leri kaldırıyoruz buradaki fonksiyon ile (Gereksiz sözcükler çünkü)
def token(values):
    filtered_words = [word for word in values.split() if word not in stop_word_list]
    
    not_stopword_doc = " ".join(filtered_words)
    return not_stopword_doc
docs = docs.map(lambda x: token(x))
df = docs
print(df.head(1))

bow_vector = dictionary.doc2bow(preprocess(df[0]))

count=0
for index, score in sorted(lda[bow_vector], key=lambda tup: -1*tup[1]):
    count+=1
    if(count<6):
      print("Score: {}\t Topic: {}".format(score, lda.print_topic(index, 5)))