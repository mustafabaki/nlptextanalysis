# -*- coding: utf-8 -*-
"""nmf_tr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X4jkiwWz_12mjqRq4ivVT2xvFB69yb97
"""

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF
import pandas as pd
import joblib

df = pd.read_csv('/content/drive/MyDrive/clean_data_turkce.csv')

content = df['text'].astype(str)
#import pickle
#content = pickle.load( open( "/content/drive/MyDrive/data_tr_nmf.pkl", "rb" ) )

# clean the data

import spacy
import re
import string
def clean_text(text):
    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''
    text = text.lower()
    text = re.sub(r'\[.*?\!?();:$%&#]', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\w*\d\w*', '', text)
    return text

def combine(txt):
    """
    combine words
    """
    temp = []
    for word in txt:
        temp.append(word)
        temp.append(" ")
    return "".join(temp)

stopwords_txt = "ancak,varmak,tak,kö,de,yok,olmak,olup,acaba,ama,aslında,az,bazı,belki,biri,birkaç,birşey,biz,bu,çok,çünkü,da,daha,de,defa,diye,eğer,en,gibi,hem,hep,hepsi,her,hiç,için,ile,ise,kez,ki,kim,mı,mu,mü,nasıl,ne,neden,nerde,nerede,nereye,niçin,niye,o,sanki,şey,siz,şu,tüm,ve,veya,ya,yani,’,-"
initial_stopwords = stopwords_txt.split(",")
stopwords = []
for text in initial_stopwords:
  stopwords.append(text)

import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
import nltk
def stemmer_fun_stop(sentence,stopwords):
    """
    remove stopwords from the given sentence
    """
    token_words=word_tokenize(sentence)
    stem_sentence=[]
    for word in token_words:
      if word not in stopwords:
        if len(word)>2:
          stem_sentence.append(word)
          stem_sentence.append(" ") 
    return "".join(stem_sentence)

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF
# NMF is able to use tf-idf
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, ngram_range=(1,2)) 
tfidf = tfidf_vectorizer.fit_transform(content)
tfidf_feature_names = tfidf_vectorizer.get_feature_names()
# Run NMF
import joblib
nmf = joblib.load('/content/drive/MyDrive/finalized_model_tr_son.sav')
#nmf = NMF(n_components=20, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)

 
def display_topics(model, feature_names, no_top_words):
    """
    To display words in each topic with desc. order
    """
    for topic_idx, topic in enumerate(model.components_):
        print ("Topic %d:" % (topic_idx))
        print (" ".join([feature_names[i]
                        for i in topic.argsort()[:-no_top_words - 1:-1]]))
        
no_top_words = 10
#display_topics(nmf, tfidf_feature_names, no_top_words)

# pip install zeyrek
import zeyrek
import nltk
analyzer = zeyrek.MorphAnalyzer()

def lemmatization(texts, allowed_postags=["Noun", 'Adj', 'Verb', 'Adv']):
      """
      turkish lemmatizer 
      """
      texts_out = []
      text = texts.split(" ")
      for sent in text:
        if sent !="":
          x=analyzer.analyze(sent)[0][0]
          if (x.pos=="Unk"):
            texts_out.append(analyzer.lemmatize(sent)[0][1][0])
          else:
            texts_out.append(x.lemma)
      return texts_out


import pandas as pd

#Sample 
sample = # thetext 
sample_clean = clean_text(sample)
sample_lem = stemmer_fun_stop(sample_clean,stopwords)
#sample_all_clean = lemmatization(sample_lem, allowed_postags=['Noun', 'Adj', 'Verb', 'Adv'])
sample_all_clean = combine(sample_lem)

# Transform the TF-IDF
test = tfidf_vectorizer.transform([sample_all_clean])
#  Transform the TF-IDF: nmf_features
nmf_features = nmf.transform(test)


def display_topic_of_sample(model, feature_names, no_top_words, topic_name):
    for topic_idx, topic in enumerate(model.components_):
      if topic_name==topic_idx:
        print ("Topic %d:" % (topic_idx))
        print (" ".join([feature_names[i]
                        for i in topic.argsort()[:-no_top_words - 1:-1]]))

#display_topic_of_sample(nmf, tfidf_feature_names, no_top_words, nmf.transform(test).argmax(axis=1))


def display_topics_of_sample(model, feature_names, no_top_words, topic_names , prct):
  y=19
  for x in range(5):
    topic_name = topic_names[0][y]
    y = y-1  
    for topic_idx, topic in enumerate(model.components_):
      if topic_name == topic_idx:
        print ("Topic percentage %" , prct[0][topic_name])
        print (" ".join([feature_names[i]
                        for i in topic.argsort()[:-no_top_words - 1:-1]]))

prct = nmf.transform(test)*100

#make list
def make_list(model, feature_names, no_top_words, topic_names , prct):
    lst = []
    y= 19
    for x in range(5):
        topic_name = topic_names[0][y]
        y = y-1  
        for topic_idx, topic in enumerate(model.components_):
            if topic_name == topic_idx:
                lst.append( [prct[0][topic_name], " ".join([feature_names[i]
                                for i in topic.argsort()[:-no_top_words - 1:-1]])])
    return lst
prct = nmf.transform(test)*100
List = make_list(nmf, tfidf_feature_names, no_top_words, nmf.transform(test).argsort(axis=1), prct)

rslt = []
print(List)
print(len(List))
for index in range(len(List)):
  theresult = result(List[index][1], List[index][0])
  rslt.append(theresult)
