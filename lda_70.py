# -*- coding: utf-8 -*-
"""LDA Wiki 70 (With Saved Model).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VMwG7TOaRj7OaX18nlV0aLQqa02x1u0G
"""

!pip install gensim
import gensim
from gensim.utils import simple_preprocess

import pickle
dictionary = gensim.corpora.Dictionary.load('/content/drive/MyDrive/nlp wiki 70 topic/dictionary.gensim')
corpus = pickle.load(open('/content/drive/MyDrive/nlp wiki 70 topic/corpus.pkl', 'rb'))
lda = gensim.models.ldamodel.LdaModel.load('/content/drive/MyDrive/nlp wiki 70 topic/model_70_topic.gensim')

import pandas as pd
from snowballstemmer import TurkishStemmer
stemmer=TurkishStemmer()
import nltk
nltk.download('stopwords')
WPT = nltk.WordPunctTokenizer()
stop_word_list = nltk.corpus.stopwords.words('turkish')
def lemmatize_stemming(text):
    return stemmer.stemWord(text)

# Tokenize and lemmatize
def preprocess(text):
    result=[]
    for token in gensim.utils.simple_preprocess(text) :
        if token not in stop_word_list and len(token) > 3:
            result.append(lemmatize_stemming(token))
     
    return result
unseen_document = ["Çinden gelen bu bulaşıcı virüs, tüm insanlığı harekete geçirdi"]
# Data preprocessing step for the unseen document

import nltk
nltk.download('stopwords')
WPT = nltk.WordPunctTokenizer()
stop_word_list = nltk.corpus.stopwords.words('turkish')
#Daha sonra verimizdeki noktalama işaretlerini kaldırıp stopword'lerden arındırıyoruz.
from pandas import DataFrame
df = DataFrame(unseen_document,columns=['text'])
docs = df['text']
docs = docs.map(lambda x: re.sub('[,\.!?();:$%&#"]', '', x))
docs = docs.map(lambda x: x.lower())
docs = docs.map(lambda x: x.strip())
#stopword'leri kaldırıyoruz buradaki fonksiyon ile (Gereksiz sözcükler çünkü)
def token(values):
    filtered_words = [word for word in values.split() if word not in stop_word_list]
    
    not_stopword_doc = " ".join(filtered_words)
    return not_stopword_doc
docs = docs.map(lambda x: token(x))
df = docs
print(df.head(1))

bow_vector = dictionary.doc2bow(preprocess(df[0]))


for index, score in sorted(lda[bow_vector], key=lambda tup: -1*tup[1]):
    print("Score: {}\t Topic: {}".format(score, lda.print_topic(index, 5)))