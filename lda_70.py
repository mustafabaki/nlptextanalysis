class result:
    """
    Holds topic names and their proportions
    """"
    def __init__(self, topic, score):
        self.topic = topic
        self.score = score

from zeyrek.morphology import MorphAnalyzer
import gensim
import re
import pandas as pd
from gensim.utils import simple_preprocess
import zeyrek
import pickle




def make_ngrams(texts,n,ngram_mod):
    """
    Sends each text to turmod function and returns its outputs as a list
    """
    return [turnmod(texts[0],n,ngram_mod)]


def clean(df):
    """
    Cleans the data from some noisy elements (it can be change according to the data sent)
    """
  df.fillna('').astype(str)
  df=df.astype(str)
  df = df.map(lambda x: re.sub('[,\.!?();:$%&#"]', '', x))
  df = df.replace('\n','', regex=True)                     # depends on the data
  df = df.replace('\'','', regex=True)                     # depends on the data
  df = df.replace('-','', regex=True)                      # depends on the data
  df = df.replace('â€™','', regex=True)
  return df 

def prepare_stopwords(link='stopwords.csv'):
    """
    Creates stopword list. If there is no specialized stopwors, then it uses default document.
    """"
  stop_word_list=pd.read_csv(link)
  stop_word_list=stop_word_list.values.tolist()
  stopwords=[]
  for i in stop_word_list:
    stopwords.append(i[0])
  return stopwords

def turnmod(text,n,ngram_mod):
    """
    Completes ngrams cumulatively according to the amount of given ngrams
    """
    data_gram=ngram_mod[0][text]
    for i in range(n-1):
      data_gram=ngram_mod[i+1][data_gram]
    return data_gram

def preprocess(text,stopwords):
    """
    Filters the words according to stopwords and their length.
    """
    result=[]
    for token in gensim.utils.simple_preprocess(text) :
        if token not in stopwords and len(token) > 3:
            result.append(token)
     
    return result

# -*- coding: utf-8 -*-
"""LDA Wiki 70 (With Saved Model).ipynb

    Automatically generated by Colaboratory.

    Original file is located at
    https://colab.research.google.com/drive/1VMwG7TOaRj7OaX18nlV0aLQqa02x1u0G
"""

#pip install gensim
def lda_70(thetext,ngramNumber,isNgramb,isStopwords,stpwrd_path):
    
    """
    Main function to be called in interface
    """
    
    analyzer=MorphAnalyzer()
    if (isStopwords):
        stopwords=prepare_stopwords(stpwrd_path)
    else:
        stopwords=prepare_stopwords()

    if (ngramNumber==1 or (isNgramb==False)):
        dictionary = gensim.corpora.Dictionary.load('turk_dictionary1.gensim')
        corpus = pickle.load(open('turk_corpus1.pkl', 'rb'))
        lda = gensim.models.ldamodel.LdaModel.load('turk_lda1.gensim')
    elif (ngramNumber==2):
        dictionary = gensim.corpora.Dictionary.load('turk_dictionary2.gensim')
        corpus = pickle.load(open('turk_corpus2.pkl', 'rb'))
        lda = gensim.models.ldamodel.LdaModel.load('turk_lda2.gensim')
    elif (ngramNumber==3):
        dictionary = gensim.corpora.Dictionary.load('turk_dictionary3.gensim')
        corpus = pickle.load(open('turk_corpus3.pkl', 'rb'))
        lda = gensim.models.ldamodel.LdaModel.load('turk_lda3.gensim')

    import pandas as pd
    import nltk
    nltk.download('stopwords')
    WPT = nltk.WordPunctTokenizer()
    
    
    """Data preprocessing step for the unseen document
    """
    unseen_document = [thetext]
    import nltk
    nltk.download('stopwords')
    WPT = nltk.WordPunctTokenizer()
    from pandas import DataFrame
    df = DataFrame(unseen_document,columns=['text'])
    df = df['text']
    df=clean(df)

    processed_docs = []

    for doc in df:
        processed_docs.append(preprocess(doc,stopwords))
    if ((isNgramb) or (not ngramNumber==1)):
        ngram=[]
        ngram_mod=[]
        for i in range(3):
            if(i==0):
                ngram.append(gensim.models.Phrases(processed_docs, min_count=5, threshold=100)) # higher threshold fewer phrases
            else:
                ngram.append(gensim.models.Phrases(ngram[i-1][processed_docs], min_count=5, threshold=100)) # higher threshold fewer phrases
            ngram_mod.append(gensim.models.phrases.Phraser(ngram[i]))

    if ((isNgramb) or (not ngramNumber==1)):
    # Form Ngrams
        data_words_ngrams = make_ngrams(processed_docs,ngramNumber,ngram_mod)[0]
        # Do lemmatization keeping only noun, adj, vb, adv

        data_lemmatized = lemmatization(data_words_ngrams,analyzer)
    else:
        data_lemmatized=processed_docs


    bow_vector = dictionary.doc2bow(data_lemmatized)

    topics = []


    for index, score in sorted(lda[bow_vector], key=lambda tup: -1*tup[1]):
        print("Score: {}\t Topic: {}".format(score, lda.print_topic(index, 5)))
    
        rslt = result(str(score), str(re.findall('"([^"]*)"', str(lda.print_topic(index,5)))))
        topics.append(rslt)

    
    return topics

def lemmatization(texts,analyzer):
    """
    Checks each word, make them lemmatized and returns lemmatized words as strings by using Morph Analyzer
    """
      texts_out = []
      for sent in texts:
          x=analyzer.analyze(sent)[0][0]
          if (x.pos=="Unk"):
            texts_out.append(analyzer.lemmatize(sent)[0][1][0])
          else:
            texts_out.append(x.lemma)
      return texts_out
