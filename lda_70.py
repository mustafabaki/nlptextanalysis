class result:
    def __init__(self, topic, score):
        self.topic = topic
        self.score = score


# -*- coding: utf-8 -*-
"""LDA Wiki 70 (With Saved Model).ipynb

    Automatically generated by Colaboratory.

    Original file is located at
    https://colab.research.google.com/drive/1VMwG7TOaRj7OaX18nlV0aLQqa02x1u0G
"""

#pip install gensim
def lda70(thetext):
    import gensim
    import re
    from gensim.utils import simple_preprocess

    import pickle
    dictionary = gensim.corpora.Dictionary.load('Algorithm/NLP LDA With 70 topic/dictionary.gensim')
    corpus = pickle.load(open('Algorithm/NLP LDA With 70 topic/corpus.pkl', 'rb'))
    lda = gensim.models.ldamodel.LdaModel.load('Algorithm/NLP LDA With 70 topic/model_70_topic.gensim')

    import pandas as pd
    from snowballstemmer import TurkishStemmer
    stemmer=TurkishStemmer()
    import nltk
    nltk.download('stopwords')
    WPT = nltk.WordPunctTokenizer()
    stop_word_list = nltk.corpus.stopwords.words('turkish')
    def lemmatize_stemming(text):
        return stemmer.stemWord(text)

    # Tokenize and lemmatize
    def preprocess(text):
        result=[]
        for token in gensim.utils.simple_preprocess(text) :
            if token not in stop_word_list and len(token) > 3:
                result.append(lemmatize_stemming(token))
        
        return result
    unseen_document = [thetext]
    # Data preprocessing step for the unseen document

    import nltk
    nltk.download('stopwords')
    WPT = nltk.WordPunctTokenizer()
    stop_word_list = nltk.corpus.stopwords.words('turkish')
    #Daha sonra verimizdeki noktalama işaretlerini kaldırıp stopword'lerden arındırıyoruz.
    from pandas import DataFrame
    df = DataFrame(unseen_document,columns=['text'])
    docs = df['text']
    docs = docs.map(lambda x: re.sub('[,\.!?();:$%&#"]', '', x))
    docs = docs.map(lambda x: x.lower())
    docs = docs.map(lambda x: x.strip())
    #stopword'leri kaldırıyoruz buradaki fonksiyon ile (Gereksiz sözcükler çünkü)
    def token(values):
        filtered_words = [word for word in values.split() if word not in stop_word_list]
        
        not_stopword_doc = " ".join(filtered_words)
        return not_stopword_doc
    docs = docs.map(lambda x: token(x))
    df = docs
    print(df.head(1))

    bow_vector = dictionary.doc2bow(preprocess(df[0]))

    topics = []


    for index, score in sorted(lda[bow_vector], key=lambda tup: -1*tup[1]):
        print("Score: {}\t Topic: {}".format(score, lda.print_topic(index, 5)))
    # rslt = result(str(score), str(lda.print_topic(index,5)))
        rslt = result(str(score), str(re.findall('"([^"]*)"', str(lda.print_topic(index,5)))))
        topics.append(rslt)

    return topics
