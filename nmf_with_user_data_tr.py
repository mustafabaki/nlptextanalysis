# -*- coding: utf-8 -*-
"""nmf_with_user_data_tr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uQpswAaLVJ1VQjhJhOxwsLVEbr8KGeXF
"""
class result:
    def __init__(self, topic, score):
        self.topic = topic
        self.score = score

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF
import pandas as pd
import joblib
import re
import string
import spacy
from nltk.tokenize import word_tokenize
import nltk        

def clean_text(text):
    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''
    text = text.lower()
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\w*\d\w*', '', text)
    return text

import nltk
nltk.download('punkt')
def stemmer_fun_stop(sentence,stopwords):
    """
    take sentence and stopword list and remove stopwords from the sentence
    """
    token_words=word_tokenize(sentence)
    stem_sentence=[]
    for word in token_words:
        if word not in stopwords:
            stem_sentence.append(word)
            stem_sentence.append(" ") 
    return "".join(stem_sentence)


import zeyrek
import nltk
analyzer = zeyrek.MorphAnalyzer()

def lemmatization(texts, allowed_postags=["Noun", 'Adj', 'Verb', 'Adv']):
    """
    turkish leematizer, it is not work properly and take so much times (not recommended to use)
    if it does not run, write this command to the terminal 'pip install zeyrek'.
    """
    texts_out = []
    text = texts.split(" ")
    for sent in text:
        if sent !="":
            x=analyzer.analyze(sent)[0][0]
            if (x.pos=="Unk"):
                texts_out.append(analyzer.lemmatize(sent)[0][1][0])
            else:
                texts_out.append(x.lemma)
    return texts_out    

def combine(txt):
    """
    combine words and return one sentence
    """
    temp = []
    for word in txt:
        temp.append(word)
        temp.append(" ")
    return "".join(temp)

#make list
def make_list(model, feature_names, no_top_words, topic_names , prct, n_topic):
    """
    return list of most used words in the each topic
    """
    lst = []
    y= n_topic 
    for x in range(5):
        topic_name = topic_names[0][y]
        y = y-1  
        for topic_idx, topic in enumerate(model.components_):
            if topic_name == topic_idx:
                lst.append( [prct[0][topic_name], " ".join([feature_names[i]
                                for i in topic.argsort()[:-no_top_words - 1:-1]])])
    return lst


def display_topics(model, feature_names, no_top_words):
    """
    To display words with desc. order 
    """
    for topic_idx, topic in enumerate(model.components_):
        print ("Topic %d:" % (topic_idx))
        print (" ".join([feature_names[i]
                        for i in topic.argsort()[:-no_top_words - 1:-1]]))

def nmf_with_dataset(dataset_path, dataset_column,stopword_choice,stopwords_user, ngram_number,user_text):
  """
   dataset column refers to the column name that has to be used for nlp, stopwords user is a list includes stopwords , user text is the sample text.
  """
  #import user dataset
  path=dataset_path 
  try:
    df = pd.read_csv(path)
  except: 
    df = pd.read_csv(path, encoding = "utf-16")

  column_name = dataset_column
  content = df[column_name]

  #Cleaning Process
  pd.options.mode.chained_assignment = None  # default='warn'


  i=0
  for text in content:
    content[i]=clean_text(text)
    i=i+1

  # Get N-number and Stopwords
  stopwords_txt = "acaba,ama,aslında,az,bazı,belki,biri,birkaç,birşey,biz,bu,çok,çünkü,da,daha,de,defa,diye,eğer,en,gibi,hem,hep,hepsi,her,hiç,için,ile,ise,kez,ki,kim,mı,mu,mü,nasıl,ne,neden,nerde,nerede,nereye,niçin,niye,o,sanki,şey,siz,şu,tüm,ve,veya,ya,yani"
  initial_stopwords = stopwords_txt.split(",")

  #user's stopwords 
  stopwords = stopwords_user
  a= stopword_choice #if user does not give stopwords this variable = false
  if a: 
    #kullanıcıdan gelen stopwordsleri split ile ayırıp for döngüsünde append ile stopwords listesine ekliyoruz.
    form= stopwords_user #example
    arr = form.split(',')
    for i in arr:
      stopwords.append(i)
  else:
    for text in initial_stopwords:
      stopwords.append(text)

  #number of n
  max_n_gram = ngram_number # user input

  #number of topic
  n_topic = 10 #user input

  i=0
  for text in content:
    content[i] = stemmer_fun_stop(text,stopwords)
    i=i+1

  data_lemmatized=[]
  for text in content:
    data_lemmatized.append(lemmatization(text, allowed_postags=['Noun', 'Adj', 'Verb', 'Adv']))

  data = []

  for txt in data_lemmatized:
    data.append(combine(txt))

  # NMF is able to use tf-idf
  tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000, ngram_range=(max_n_gram,max_n_gram)) 
  tfidf = tfidf_vectorizer.fit_transform(data)
  tfidf_feature_names = tfidf_vectorizer.get_feature_names()
  # Run NMF
  nmf = NMF(n_components=n_topic, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)

          
  no_top_words = 10
  #display_topics(nmf, tfidf_feature_names, no_top_words)

  #Sample 
  sample = user_text

  sample_clean = clean_text(sample)
  sample_lem = stemmer_fun_stop(sample_clean,stopwords)
  sample_all_clean = lemmatization(sample_lem, allowed_postags=['Noun', 'Adj', 'Verb', 'Adv'])
  sample_all_clean = combine(sample_all_clean)

  # Transform the TF-IDF
  test = tfidf_vectorizer.transform([sample_all_clean])
  #  Transform the TF-IDF: nmf_features
  nmf_features = nmf.transform(test)
      
  prct = nmf.transform(test)*100
  List = make_list(nmf, tfidf_feature_names, no_top_words, nmf.transform(test).argsort(axis=1), prct, n_topic)
  rslt = []
  print(List)
  for index in range(len(List)):
    theresult = result(List[index][1], List[index][0])
    rslt.append(theresult)
  
  return rslt
